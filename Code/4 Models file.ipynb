{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predection of Mood (Fatigue and fatigue) by different Balance parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (Temp/ipykernel_4424/1523096156.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ahmed\\AppData\\Local\\Temp/ipykernel_4424/1523096156.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Path = \"C:\\Users\\ahmed\\Documents\\GitHub\\Balance_and_gait\\Data\\\"\u001b[0m\n\u001b[1;37m                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "Path = \"C:\\Users\\ahmed\\Documents\\GitHub\\Balance_and_gait\\Data\\\"\n",
    "Original_file_name = 'Original_Data.xlsx'\n",
    "y_name_list = ['CurrentPOMSFatigue', 'Cate_Med_Fatigue', 'CurrentPOMSVigor', \n",
    "'Cate_Med_Vigor', 'Cate_M_Fatigue_vigor']\n",
    "Cat_Var_drop_Nor = ['Gender1male2female']\n",
    "k_folds=2\n",
    "repetition=2\n",
    "z=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data From File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "# Path of Raw Data on the computer\n",
    "DATA_PATH = r'C:\\Users\\ahmed\\Documents\\GitHub\\Balance_and_gait\\Data\\Original_Data_after_Imput_Winser_scaled.xlsx'\n",
    "EXPORT_PATH = r'C:\\Users\\ahmed\\Documents\\GitHub\\Balance_and_gait\\Data\\Data_results.xlsx'\n",
    "EXPORT_PATH2 = r'C:\\Users\\ahmed\\Documents\\GitHub\\Balance_and_gait\\Data\\Data_results.plk'\n",
    "# import DATA after having the LABLE (Frist Columen is Y) columnes 2 and 3 are the neuomerical data of predectors form thim Y calculated)\n",
    "df = pd.read_excel(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "# import necessary libraries\n",
    "#----------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "#----------------------------------------------------\n",
    "# Import Classifiers from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#----------------------------------------------------\n",
    "# Import Regressors from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "#----------------------------------------------------\n",
    "# Import model metrics from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "#----------------------------------------------------\n",
    "# Import Confusion matrix from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#----------------------------------------------------\n",
    "# Import ROC curve from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "#----------------------------------------------------\n",
    "# Import Model evaluation metrics from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "# Classifier Models\n",
    "#----------------------------------------------------\n",
    "RandomForestClassifierModel = RandomForestClassifier(criterion = 'gini', random_state=33) #criterion can be also : entropy \n",
    "LogisticRegressionModel = LogisticRegression(penalty='l2', solver='sag',C=1.0,random_state=33)\n",
    "MLPClassifierModel = MLPClassifier(activation='tanh', solver='lbfgs',  learning_rate='constant', early_stopping= False, alpha=0.0001 ,hidden_layer_sizes=(100, 3),random_state=33)\n",
    "DecisionTreeClassifierModel = DecisionTreeClassifier(criterion='gini',random_state=33) \n",
    "SVCModel = SVC(kernel= 'rbf', probability=True, C=1.0,gamma='auto')\n",
    "KNNClassifierModel = KNeighborsClassifier(n_neighbors= 5,weights ='uniform', algorithm='auto') \n",
    "GaussianNBModel = GaussianNB()\n",
    "LDAModel = LinearDiscriminantAnalysis()\n",
    "GradientBoostingClassifierModel = GradientBoostingClassifier( learning_rate=1.0, random_state=0) \n",
    "BaggingClassifierModel = BaggingClassifier(base_estimator=SVC(),  random_state=0)\n",
    "#----------------------------------------------------\n",
    "# Regressor Models\n",
    "#----------------------------------------------------\n",
    "DecisionTreeRegressorModel = DecisionTreeRegressor(random_state=33)\n",
    "LinearRegressionModel = LinearRegression(fit_intercept=True, normalize=True,copy_X=True,n_jobs=-1)\n",
    "RandomForestRegressorModel = RandomForestRegressor(random_state=33)\n",
    "RidgeRegressionModel = Ridge(alpha=1.0, random_state=33)\n",
    "LassoRegressionModel = Lasso(alpha=1.0, random_state=33,normalize=False)\n",
    "SGDRegressionModel = SGDRegressor(alpha=0.1, random_state=33,penalty='l2',loss = 'huber')\n",
    "MLPRegressorModel = MLPRegressor(activation='tanh',solver='lbfgs',learning_rate='constant',early_stopping= False, \n",
    "                    alpha=0.0001 ,hidden_layer_sizes=(100, 3),random_state=33)\n",
    "SVRModel = SVR(C = 1.0 ,epsilon=0.1,kernel = 'rbf') \n",
    "GBRModel = GradientBoostingRegressor(learning_rate = 1.5 ,random_state=33)\n",
    "NeighborsRegressorModel = KNeighborsRegressor(n_neighbors = 5, weights='uniform',algorithm = 'auto')\n",
    "#----------------------------------------------------\n",
    "# Classifier and regressor list\n",
    "#----------------------------------------------------\n",
    "Model_list = [RandomForestClassifierModel, LogisticRegressionModel, LogisticRegressionModel, MLPClassifierModel,\n",
    "DecisionTreeClassifierModel, KNNClassifierModel, GaussianNBModel, LDAModel, \n",
    "GradientBoostingClassifierModel, DecisionTreeRegressorModel, \n",
    "RandomForestRegressorModel,RidgeRegressionModel,\n",
    "SGDRegressionModel,MLPRegressorModel,SVRModel,GBRModel,NeighborsRegressorModel]\n",
    "#----------------------------------------------------\n",
    "# y variable definition\n",
    "#----------------------------------------------------\n",
    "y1 ='Cate_Med_Fatigue'\n",
    "y2 ='Cate_Med_Vigor'\n",
    "y3 ='Cate_M_Fatigue_vigor'\n",
    "y4 ='CurrentPOMSFatigue'\n",
    "y5 ='CurrentPOMSVigor'\n",
    "df=df.astype({y1:'int', y2:'int', y3:'int', y4:'int', y5:'int'}) \n",
    "y_name_list = [y1, y2, y3, y4, y5]\n",
    "#----------------------------------------------------\n",
    "# X variable definition\n",
    "#----------------------------------------------------\n",
    "X = df.drop(['CurrentPOMSFatigue', 'Cate_Med_Fatigue', 'CurrentPOMSVigor', \n",
    "'Cate_Med_Vigor', 'Cate_M_Fatigue_vigor'], axis=1)\n",
    "X_columns_names = df.columns.drop(['CurrentPOMSFatigue', 'Cate_Med_Fatigue', 'CurrentPOMSVigor', \n",
    "'Cate_Med_Vigor', 'Cate_M_Fatigue_vigor'])\n",
    "#----------------------------------------------------\n",
    "# Create a cross validation variable\n",
    "#----------------------------------------------------\n",
    "k_folds=2\n",
    "repetition=2\n",
    "cv = RepeatedStratifiedKFold(n_splits=k_folds, n_repeats= repetition, random_state= 2)\n",
    "#----------------------------------------------------\n",
    "# Create an empty dataframe for results\n",
    "#----------------------------------------------------\n",
    "results_df = pd.DataFrame(columns= {'X_shape','y', 'important_features_list' \n",
    "'important_feature_importances', 'Model_used', 'feature_Selection_method','Modelcheck_mean', \n",
    "'Modelcheck_min', 'Modelcheck_Q1', 'Modelcheck_Q2',  \n",
    "'Modelcheck_Q3', 'Modelcheck_max','roc_F1_score_mean_score', \n",
    "'roc_auc_mean_score', 'roc_percesion_mean_score',\n",
    "'roc_sensetivity_mean_score', 'negative_log_likelihood', 'neg_log_loss'\n",
    "'r2','neg_mean_absolute_error', 'neg_mean_squared_error'}, index= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------\n",
    "# Creating for loop for y_name_list\n",
    "#----------------------------------------------------\n",
    "for y_name in y_name_list:\n",
    "    y = df[y_name].values \n",
    "    for Model_name in Model_list:\n",
    "        # Test and Train separation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=33, shuffle =False)\n",
    "        # Feature selection application\n",
    "        Model_name.fit(X, y)\n",
    "        # Get importance weights\n",
    "        from sklearn.inspection import permutation_importance\n",
    "        Model_name.score(X_test, y_test)       \n",
    "        # IF statement\n",
    "        if Model_name in [LogisticRegressionModel] :\n",
    "            importance = Model_name.coef_[0]\n",
    "            feature_Selection_method = \"the same as the regressor\"\n",
    "        elif Model_name in [GradientBoostingClassifierModel, RandomForestClassifierModel]:\n",
    "            importance = Model_name.feature_importances_\n",
    "            feature_Selection_method = \"the same as the classifier\"\n",
    "        else:\n",
    "            importance = permutation_importance (Model_name, X_test, y_test, n_repeats=5, random_state=0)\n",
    "            importance = importance.importances_mean\n",
    "            feature_Selection_method = \"permutation\"\n",
    "        # Get features names with their weights\n",
    "        feats = {} \n",
    "        for feature, importance in zip(X_columns_names, importance):\n",
    "            feats[feature] = importance  \n",
    "        importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'importance'})\n",
    "        # Sort features with top 12 important feature at the top\n",
    "        importanc_df = importances.sort_values(by ='importance' ,ascending=False).head(12)\n",
    "        # Create feature names list\n",
    "        important_features_list = importanc_df.index.tolist()\n",
    "        important_feature_importances = importanc_df.importance.tolist()\n",
    "        # Create important features dataframe\n",
    "        important_features_df = pd.DataFrame()\n",
    "        # Insert values in dataframe\n",
    "        for K in important_features_list:\n",
    "            important_features_df = important_features_df.append(df[K])\n",
    "        important_features_df = important_features_df.transpose()\n",
    "        XIF = important_features_df\n",
    "        #XIF = important_features_df.values\n",
    "        # Make X list\n",
    "        X_list = [X, XIF]\n",
    "        for Xroll in X_list:\n",
    "            X_shape = Xroll.shape\n",
    "            #Splitting data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Xroll, y, test_size=0.10, random_state=33, shuffle =True)\n",
    "            if Model_name in [RandomForestClassifierModel, LogisticRegressionModel, MLPClassifierModel,\n",
    "            DecisionTreeClassifierModel, SVCModel, KNNClassifierModel, GaussianNBModel, LDAModel, \n",
    "            GradientBoostingClassifierModel, BaggingClassifierModel]:\n",
    "                CrossValidateValues1 = cross_validate(Model_name,Xroll,y,cv=cv,return_train_score = True, n_jobs=-1)\n",
    "                Modelcheck_trainscore = CrossValidateValues1['train_score']\n",
    "                Modelcheck_testscore = CrossValidateValues1['test_score'] \n",
    "                Modelcheck_mean_testscore = CrossValidateValues1['test_score'].mean() \n",
    "                Modelcheck_min_testscore = CrossValidateValues1['test_score'].min() \n",
    "                Modelcheck_max_testscore = CrossValidateValues1['test_score'].max() \n",
    "                test_score_Q1 = np.quantile(Modelcheck_testscore, .25)\n",
    "                test_score_Q2 = np.quantile(Modelcheck_testscore, .50)\n",
    "                test_score_Q3 = np.quantile(Modelcheck_testscore, .75)\n",
    "                Modelcheck_fitscore = CrossValidateValues1['fit_time']\n",
    "                roc_F1_score_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"f1_macro\", cv = cv, n_jobs= -1).mean()\n",
    "                roc_auc_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"roc_auc_ovo\", cv = cv, n_jobs= -1).mean()\n",
    "                roc_percesion_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"precision_macro\", cv = cv, n_jobs= -1).mean()\n",
    "                roc_sensetivity_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"recall_macro\", cv = cv, n_jobs= -1).mean()\n",
    "                negative_log_likelihood1 = cross_val_score(Model_name, Xroll, y, scoring=\"neg_brier_score\", cv = cv, n_jobs= -1)\n",
    "                neg_log_loss1 = cross_val_score(Model_name, Xroll, y, scoring=\"neg_log_loss\", cv = cv, n_jobs= -1)\n",
    "                neg_log_loss = np.median(neg_log_loss1)\n",
    "                negative_log_likelihood = np.median(negative_log_likelihood1)\n",
    "                # Insert results in dataframe\n",
    "                results_df = results_df.append({'X_shape': X_shape,'y': y_name,'important_features_list': important_features_list, \n",
    "                'important_feature_importances':important_feature_importances, 'Model_used':Model_name, 'feature_Selection_method': feature_Selection_method,           \n",
    "                'Modelcheck_mean': Modelcheck_mean_testscore, 'Modelcheck_min': Modelcheck_min_testscore, \n",
    "                'Modelcheck_Q1': test_score_Q1, 'Modelcheck_Q2': test_score_Q2,'Modelcheck_Q3': test_score_Q3,\n",
    "                'Modelcheck_max': Modelcheck_max_testscore,\n",
    "                'roc_F1_score_mean_score': roc_F1_score_mean_score,'roc_auc_mean_score': roc_auc_mean_score, \n",
    "                'roc_percesion_mean_score': roc_percesion_mean_score,'roc_sensetivity_mean_score': roc_sensetivity_mean_score,\n",
    "                'negative_log_likelihood': negative_log_likelihood,'neg_log_loss': [neg_log_loss],\n",
    "                'r2': ['Null'], 'neg_mean_absolute_error': ['Null'], 'neg_mean_squared_error':['Null'] }, ignore_index=True) \n",
    "            else:\n",
    "                CrossValidateValues1 = cross_validate(Model_name,Xroll,y,cv=cv,return_train_score = True, n_jobs=-1)\n",
    "                explained_variance = cross_val_score(Model_name, Xroll, y, scoring=\"explained_variance\", cv = cv, n_jobs= -1).mean()\n",
    "                Modelcheck_mean_testscore = explained_variance.mean() \n",
    "                Modelcheck_min_testscore = explained_variance.min() \n",
    "                Modelcheck_max_testscore = explained_variance.max() \n",
    "                test_score_Q1 = np.quantile(explained_variance, .25)\n",
    "                test_score_Q2 = np.quantile(explained_variance, .50)\n",
    "                test_score_Q3 = np.quantile(explained_variance, .75)\n",
    "                Modelcheck_fitscore = CrossValidateValues1['fit_time']\n",
    "                r2 = cross_val_score(Model_name, Xroll, y, scoring=\"r2\", cv = cv, n_jobs= -1).mean()\n",
    "                mae = cross_val_score(Model_name, Xroll, y, scoring=\"neg_mean_absolute_error\", cv = cv, n_jobs= -1).mean()\n",
    "                msqe = cross_val_score(Model_name, Xroll, y, scoring=\"neg_mean_squared_error\", cv = cv, n_jobs= -1).mean()\n",
    "                # Insert results in dataframe\n",
    "                results_df = results_df.append({'X_shape': X_shape,'y': y_name,'important_features_list': important_features_list, \n",
    "                'important_feature_importances':important_feature_importances, 'Model_used':Model_name,  'feature_Selection_method': feature_Selection_method,          \n",
    "                'Modelcheck_mean': Modelcheck_mean_testscore,'Modelcheck_min': Modelcheck_min_testscore, \n",
    "                'Modelcheck_Q1': test_score_Q1, 'Modelcheck_Q2': test_score_Q2,'Modelcheck_Q3': test_score_Q3,\n",
    "                'Modelcheck_max': Modelcheck_max_testscore,\n",
    "                'roc_F1_score_mean_score': ['Null'], 'roc_auc_mean_score': ['Null'], 'roc_percesion_mean_score': ['Null'],\n",
    "                'roc_sensetivity_mean_score': ['Null'], 'negative_log_likelihood': ['Null'], 'neg_log_loss': ['Null'],\n",
    "                'r2': r2, 'neg_mean_absolute_error': mae, 'neg_mean_squared_error':msqe }, ignore_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_excel(EXPORT_PATH, sheet_name='Sheet_name_1')  \n",
    "results_df.to_pickle(EXPORT_PATH2)  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "536656feb4e5b8426698a6ed7c3ac3e4cf6780e288841fc3960502355480ee44"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
