{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predection of Mood (Fatigue and fatigue) by different Balance parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique variables (just change these variables and run the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH = 'C:/Users/ahmed/Desktop/Python project'\n",
    "Filename = \"/Original_Data.xlsx\"\n",
    "y_name_class = ['Cate_Med_Fatigue', 'Cate_Med_Vigor', 'Cate_M_Fatigue_vigor']\n",
    "y_name_regr = ['CurrentPOMSFatigue', 'CurrentPOMSVigor']\n",
    "Cat_Var_drop_Nor = ['Gender1male2female']\n",
    "y_name_list = y_name_class + y_name_regr\n",
    "Droped_var = y_name_class + Cat_Var_drop_Nor\n",
    "k_folds=2\n",
    "repetition=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imputation and winsoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Neccesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of Raw Data on the computer\n",
    "DATA_PATH = (PATH + Filename)\n",
    "EXPORT_PATH = (PATH + '\\Original_Data_after_Imput_Winser.xlsx')\n",
    "EXPORT_PATH2 = (PATH + '\\Original_Data_after_Imput_Winser.plk')\n",
    "# import DATA after having the LABLE (Frist Columen is Y) columnes 2 and 3 are the neuomerical data of predectors form thim Y calculated)\n",
    "df = pd.read_excel(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Missed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data imputation to repalce missing data with relivant data point\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "# create an object for KNNImputer\n",
    "imputer = IterativeImputer()\n",
    "df_imput = imputer.fit_transform(df)\n",
    "# Change Data From np.array to pd.DataFrame\n",
    "df_imput = pd.DataFrame(df_imput)\n",
    "# Rename all columes from 01234 to original columes names\n",
    "df_imput.columns = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outliers form data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Winsorizing (Remove of Outliers) \n",
    "from scipy.stats.mstats import winsorize\n",
    "lower_limit = 0.0\n",
    "for j in df:\n",
    "    df_imput[j] = winsorize(df_imput[j],limits = [lower_limit, 0.05])\n",
    "df_imput_wins = df_imput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export imputated and winsored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imput_wins.to_excel(EXPORT_PATH, index=False)\n",
    "df_imput_wins.to_pickle(EXPORT_PATH2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of Raw Data on the computer\n",
    "DATA_PATH = (PATH + '\\Original_Data_after_Imput_Winser.xlsx')\n",
    "EXPORT_PATH = (PATH + '\\Original_Data_after_Imput_Winser_scaled.xlsx')\n",
    "EXPORT_PATH2 = (PATH + '\\Original_Data_after_Imput_Winser_scaled.plk')\n",
    "# import DATA after having the LABLE (Frist Columen is Y) columnes 2 and 3 are the neuomerical data of predectors form thim Y calculated)\n",
    "df_imput_winsor = pd.read_excel(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Adjustement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imput_winsor = df_imput_winsor[df_imput_winsor['Age'] <= 36]\n",
    "df_imput_winsor = df_imput_winsor.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having fixed variables alone (Y variables)\n",
    "df_imput_winsor_scal_X = df_imput_winsor.drop(Droped_var, axis=1)\n",
    "# Store the removed variables to be attached at the end of cleaning prosses\n",
    "cat_data = df_imput_winsor[Droped_var]\n",
    "# Apply Stander Scaller for the DataFrame (Normalization)\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# # perform a robust scaler transform of the dataset\n",
    "trans = Normalizer()\n",
    "X_imput_winsor_Scaled = trans.fit_transform(df_imput_winsor_scal_X)\n",
    "# convert the array back to a dataframe\n",
    "X_imput_winsor_Scaled = pd.DataFrame(X_imput_winsor_Scaled)\n",
    "X_imput_winsor_Scaled.columns = df_imput_winsor_scal_X.columns\n",
    "# Concatenate Y variabels with X_imput_scaled features in ONE data farme\n",
    "cat_data\n",
    "X_imput_winsor_Scaled\n",
    "df_imput_winsor_scal = pd.concat([cat_data,X_imput_winsor_Scaled], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Prossesed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export Data \n",
    "df_imput_winsor_scal.to_excel(EXPORT_PATH, index=False)\n",
    "df_imput_winsor_scal.to_pickle(EXPORT_PATH2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recrussive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of Raw Data on the computer\n",
    "DATA_PATH = (PATH + '\\Original_Data_after_Imput_Winser_scaled.xlsx')\n",
    "# import DATA after having the LABLE (Frist Columen is Y) columnes 2 and 3 are the neuomerical data of predectors form thim Y calculated)\n",
    "df = pd.read_excel(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "# y variable definition\n",
    "#----------------------------------------------------\n",
    "for y in y_name_list:\n",
    "    df = df.astype({y:'int'})\n",
    "#----------------------------------------------------\n",
    "# X variable definition\n",
    "#----------------------------------------------------\n",
    "X = df.drop(y_name_list, axis=1)\n",
    "X_columns_names = df.columns.drop(y_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Recursive feature elimination and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    "estimator = SVR(kernel=\"linear\")\n",
    "selector = RFE(estimator, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "z=selector.n_features_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data From File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "# Path of Raw Data on the computer\n",
    "DATA_PATH = (PATH + '\\Original_Data_after_Imput_Winser_scaled.xlsx')\n",
    "EXPORT_PATH_Class = (PATH + '\\Class_Data_results.xlsx')\n",
    "EXPORT_PATH_Regr = (PATH + '\\Regre_Data_results.xlsx')\n",
    "EXPORT_PATH2 = (PATH + '\\Data_results.plk')\n",
    "# import DATA after having the LABLE (Frist Columen is Y) columnes 2 and 3 are the neuomerical data of predectors form thim Y calculated)\n",
    "df = pd.read_excel(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "# import necessary libraries\n",
    "#----------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "#----------------------------------------------------\n",
    "# Import Classifiers from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#----------------------------------------------------\n",
    "# Import Regressors from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "#----------------------------------------------------\n",
    "# Import model metrics from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "#----------------------------------------------------\n",
    "# Import Confusion matrix from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#----------------------------------------------------\n",
    "# Import ROC curve from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "#----------------------------------------------------\n",
    "# Import Model evaluation metrics from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "# Classifier Models\n",
    "#----------------------------------------------------\n",
    "RandomForestClassifierModel = RandomForestClassifier(criterion = 'gini', random_state=33) #criterion can be also : entropy \n",
    "LogisticRegressionModel = LogisticRegression(penalty='l2', solver='sag',C=1.0,random_state=33)\n",
    "MLPClassifierModel = MLPClassifier(activation='tanh', solver='lbfgs',  learning_rate='constant', early_stopping= False, alpha=0.0001 ,hidden_layer_sizes=(100, 3),random_state=33)\n",
    "DecisionTreeClassifierModel = DecisionTreeClassifier(criterion='gini',random_state=33) \n",
    "SVCModel = SVC(kernel= 'rbf', probability=True, C=1.0,gamma='auto')\n",
    "KNNClassifierModel = KNeighborsClassifier(n_neighbors= 5,weights ='uniform', algorithm='auto') \n",
    "GaussianNBModel = GaussianNB()\n",
    "LDAModel = LinearDiscriminantAnalysis()\n",
    "GradientBoostingClassifierModel = GradientBoostingClassifier( learning_rate=1.0, random_state=0) \n",
    "BaggingClassifierModel = BaggingClassifier(base_estimator=SVC(),  random_state=0)\n",
    "#----------------------------------------------------\n",
    "# Regressor Models\n",
    "#----------------------------------------------------\n",
    "DecisionTreeRegressorModel = DecisionTreeRegressor(random_state=33)\n",
    "LinearRegressionModel = LinearRegression(fit_intercept=True, normalize=True,copy_X=True,n_jobs=-1)\n",
    "RandomForestRegressorModel = RandomForestRegressor(random_state=33)\n",
    "RidgeRegressionModel = Ridge(alpha=1.0, random_state=33)\n",
    "LassoRegressionModel = Lasso(alpha=1.0, random_state=33,normalize=False)\n",
    "SGDRegressionModel = SGDRegressor(alpha=0.1, random_state=33,penalty='l2',loss = 'huber')\n",
    "MLPRegressorModel = MLPRegressor(activation='tanh',solver='lbfgs',learning_rate='constant',early_stopping= False, \n",
    "                    alpha=0.0001 ,hidden_layer_sizes=(100, 3),random_state=33)\n",
    "SVRModel = SVR(C = 1.0 ,epsilon=0.1,kernel = 'rbf') \n",
    "GBRModel = GradientBoostingRegressor(learning_rate = 1.5 ,random_state=33)\n",
    "NeighborsRegressorModel = KNeighborsRegressor(n_neighbors = 5, weights='uniform',algorithm = 'auto')\n",
    "#----------------------------------------------------\n",
    "# Classifier and regressor list\n",
    "#----------------------------------------------------\n",
    "Class_Model_list = [RandomForestClassifierModel, LogisticRegressionModel, LogisticRegressionModel, MLPClassifierModel,\n",
    "DecisionTreeClassifierModel, KNNClassifierModel, GaussianNBModel, LDAModel, \n",
    "GradientBoostingClassifierModel]\n",
    "Regre_Model_list = [DecisionTreeRegressorModel, \n",
    "RandomForestRegressorModel,RidgeRegressionModel,\n",
    "SGDRegressionModel,MLPRegressorModel,SVRModel,GBRModel,NeighborsRegressorModel]\n",
    "#----------------------------------------------------\n",
    "# y variable definition\n",
    "#----------------------------------------------------\n",
    "y_name_class = y_name_class\n",
    "y_name_regr = y_name_regr\n",
    "for y in y_name_list:\n",
    "    df = df.astype({y:'int'})\n",
    "#----------------------------------------------------\n",
    "# X variable definition\n",
    "#----------------------------------------------------\n",
    "X = df.drop(y_name_list, axis=1)\n",
    "X_columns_names = df.columns.drop(y_name_list)\n",
    "#----------------------------------------------------\n",
    "# Create a cross validation variable\n",
    "#----------------------------------------------------\n",
    "cv = RepeatedStratifiedKFold(n_splits=k_folds, n_repeats= repetition, random_state= 2)\n",
    "#----------------------------------------------------\n",
    "# Create an empty dataframe for results\n",
    "#----------------------------------------------------\n",
    "results_df_Class = pd.DataFrame(columns= {'X_shape','y', 'important_features_list' \n",
    "'important_feature_importances', 'Model_used', 'feature_Selection_method','Modelcheck_mean', \n",
    "'Modelcheck_min', 'Modelcheck_Q1', 'Modelcheck_Q2',  \n",
    "'Modelcheck_Q3', 'Modelcheck_max','roc_F1_score_mean_score', \n",
    "'roc_auc_mean_score', 'roc_percesion_mean_score',\n",
    "'roc_sensetivity_mean_score', 'negative_log_likelihood', 'neg_log_loss'\n",
    "'r2','neg_mean_absolute_error', 'neg_mean_squared_error'}, index= None)\n",
    "results_df_regre = pd.DataFrame(columns= {'X_shape','y', 'important_features_list' \n",
    "'important_feature_importances', 'Model_used', 'feature_Selection_method','Modelcheck_mean', \n",
    "'Modelcheck_min', 'Modelcheck_Q1', 'Modelcheck_Q2',  \n",
    "'Modelcheck_Q3', 'Modelcheck_max','roc_F1_score_mean_score', \n",
    "'roc_auc_mean_score', 'roc_percesion_mean_score',\n",
    "'roc_sensetivity_mean_score', 'negative_log_likelihood', 'neg_log_loss'\n",
    "'r2','Adj_r2' ,'neg_mean_absolute_error', 'neg_mean_squared_error'}, index= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------\n",
    "# Creating for loop for y_name_list for classifier\n",
    "#----------------------------------------------------\n",
    "for y_name in y_name_class:\n",
    "    y = df[y_name].values \n",
    "    for Model_name in Class_Model_list:\n",
    "        # Test and Train separation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=33, shuffle =False)\n",
    "        # Feature selection application\n",
    "        Model_name.fit(X, y)\n",
    "        # Get importance weights\n",
    "        from sklearn.inspection import permutation_importance\n",
    "        Model_name.score(X_test, y_test)       \n",
    "        # IF statement\n",
    "        if Model_name in [LogisticRegressionModel] :\n",
    "            importance = Model_name.coef_[0]\n",
    "            feature_Selection_method = \"the same as the regressor\"\n",
    "        elif Model_name in [GradientBoostingClassifierModel, RandomForestClassifierModel]:\n",
    "            importance = Model_name.feature_importances_\n",
    "            feature_Selection_method = \"the same as the classifier\"\n",
    "        else:\n",
    "            importance = permutation_importance (Model_name, X_test, y_test, n_repeats=5, random_state=33)\n",
    "            importance = importance.importances_mean\n",
    "            feature_Selection_method = \"permutation\"\n",
    "        # Get features names with their weights\n",
    "        feats = {} \n",
    "        for feature, importance in zip(X_columns_names, importance):\n",
    "            feats[feature] = importance  \n",
    "        importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'importance'})\n",
    "        # Sort features with top 12 important feature at the top\n",
    "        importanc_df = importances.sort_values(by ='importance' ,ascending=False).head(z)\n",
    "        # Create feature names list\n",
    "        important_features_list = importanc_df.index.tolist()\n",
    "        important_feature_importances = importanc_df.importance.tolist()\n",
    "        # Create important features dataframe\n",
    "        important_features_df = pd.DataFrame()\n",
    "        # Insert values in dataframe\n",
    "        for K in important_features_list:\n",
    "            important_features_df = important_features_df.append(df[K])\n",
    "        important_features_df = important_features_df.transpose()\n",
    "        XIF = important_features_df\n",
    "        #XIF = important_features_df.values\n",
    "        # Make X list\n",
    "        X_list = [X, XIF]\n",
    "        for Xroll in X_list:\n",
    "            X_shape = Xroll.shape\n",
    "            #Splitting data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Xroll, y, test_size=0.10, random_state=33, shuffle =True)\n",
    "            CrossValidateValues1 = cross_validate(Model_name,Xroll,y,cv=cv,return_train_score = True, n_jobs=-1)\n",
    "            Modelcheck_trainscore = CrossValidateValues1['train_score']\n",
    "            Modelcheck_testscore = CrossValidateValues1['test_score'] \n",
    "            Modelcheck_mean_testscore = CrossValidateValues1['test_score'].mean() \n",
    "            Modelcheck_min_testscore = CrossValidateValues1['test_score'].min() \n",
    "            Modelcheck_max_testscore = CrossValidateValues1['test_score'].max() \n",
    "            test_score_Q1 = np.quantile(Modelcheck_testscore, .25)\n",
    "            test_score_Q2 = np.quantile(Modelcheck_testscore, .50)\n",
    "            test_score_Q3 = np.quantile(Modelcheck_testscore, .75)\n",
    "            Modelcheck_fitscore = CrossValidateValues1['fit_time']\n",
    "            roc_F1_score_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"f1_macro\", cv = cv, n_jobs= -1).mean()\n",
    "            roc_auc_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"roc_auc_ovo\", cv = cv, n_jobs= -1).mean()\n",
    "            roc_percesion_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"precision_macro\", cv = cv, n_jobs= -1).mean()\n",
    "            roc_sensetivity_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"recall_macro\", cv = cv, n_jobs= -1).mean()\n",
    "            negative_log_likelihood1 = cross_val_score(Model_name, Xroll, y, scoring=\"neg_brier_score\", cv = cv, n_jobs= -1)\n",
    "            neg_log_loss1 = cross_val_score(Model_name, Xroll, y, scoring=\"neg_log_loss\", cv = cv, n_jobs= -1)\n",
    "            neg_log_loss = np.median(neg_log_loss1)\n",
    "            negative_log_likelihood = np.median(negative_log_likelihood1)\n",
    "            # Insert results in dataframe\n",
    "            results_df_Class = results_df_Class.append({'X_shape': X_shape,'y': y_name,'important_features_list': important_features_list, \n",
    "            'important_feature_importances':important_feature_importances, 'Model_used':Model_name, 'feature_Selection_method': feature_Selection_method,           \n",
    "            'Modelcheck_mean': Modelcheck_mean_testscore, 'Modelcheck_min': Modelcheck_min_testscore, \n",
    "            'Modelcheck_Q1': test_score_Q1, 'Modelcheck_Q2': test_score_Q2,'Modelcheck_Q3': test_score_Q3,\n",
    "            'Modelcheck_max': Modelcheck_max_testscore,\n",
    "            'roc_F1_score_mean_score': roc_F1_score_mean_score,'roc_auc_mean_score': roc_auc_mean_score, \n",
    "            'roc_percesion_mean_score': roc_percesion_mean_score,'roc_sensetivity_mean_score': roc_sensetivity_mean_score,\n",
    "            'negative_log_likelihood': negative_log_likelihood,'neg_log_loss': [neg_log_loss],\n",
    "            'r2': ['Null'], 'neg_mean_absolute_error': ['Null'], 'neg_mean_squared_error':['Null'] }, ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "# Creating for loop for y_name_list for regressors\n",
    "#----------------------------------------------------\n",
    "for y_name in y_name_regr:\n",
    "    y = df[y_name].values \n",
    "    for Model_name in Regre_Model_list:\n",
    "        # Test and Train separation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=33, shuffle =False)\n",
    "        # Feature selection application\n",
    "        Model_name.fit(X, y)\n",
    "        # Get importance weights\n",
    "        from sklearn.inspection import permutation_importance\n",
    "        Model_name.score(X_test, y_test)       \n",
    "        # IF statement\n",
    "        if Model_name in [LogisticRegressionModel] :\n",
    "            importance = Model_name.coef_[0]\n",
    "            feature_Selection_method = \"the same as the regressor\"\n",
    "        elif Model_name in [GradientBoostingClassifierModel, RandomForestClassifierModel]:\n",
    "            importance = Model_name.feature_importances_\n",
    "            feature_Selection_method = \"the same as the classifier\"\n",
    "        else:\n",
    "            importance = permutation_importance (Model_name, X_test, y_test, n_repeats=5, random_state=33)\n",
    "            importance = importance.importances_mean\n",
    "            feature_Selection_method = \"permutation\"\n",
    "        # Get features names with their weights\n",
    "        feats = {} \n",
    "        for feature, importance in zip(X_columns_names, importance):\n",
    "            feats[feature] = importance  \n",
    "        importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'importance'})\n",
    "        # Sort features with top 12 important feature at the top\n",
    "        importanc_df = importances.sort_values(by ='importance' ,ascending=False).head(z)\n",
    "        # Create feature names list\n",
    "        important_features_list = importanc_df.index.tolist()\n",
    "        important_feature_importances = importanc_df.importance.tolist()\n",
    "        # Create important features dataframe\n",
    "        important_features_df = pd.DataFrame()\n",
    "        # Insert values in dataframe\n",
    "        for K in important_features_list:\n",
    "            important_features_df = important_features_df.append(df[K])\n",
    "        important_features_df = important_features_df.transpose()\n",
    "        XIF = important_features_df\n",
    "        #XIF = important_features_df.values\n",
    "        # Make X list\n",
    "        X_list = [X, XIF]\n",
    "        for Xroll in X_list:\n",
    "            X_shape = Xroll.shape\n",
    "            #Splitting data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Xroll, y, test_size=0.10, random_state=33, shuffle =True)\n",
    "            CrossValidateValues1 = cross_validate(Model_name,Xroll,y,cv=cv,return_train_score = True, n_jobs=-1)\n",
    "            explained_variance = cross_val_score(Model_name, Xroll, y, scoring=\"explained_variance\", cv = cv, n_jobs= -1).mean()\n",
    "            Modelcheck_mean_testscore = explained_variance.mean() \n",
    "            Modelcheck_min_testscore = explained_variance.min() \n",
    "            Modelcheck_max_testscore = explained_variance.max() \n",
    "            test_score_Q1 = np.quantile(explained_variance, .25)\n",
    "            test_score_Q2 = np.quantile(explained_variance, .50)\n",
    "            test_score_Q3 = np.quantile(explained_variance, .75)\n",
    "            Modelcheck_fitscore = CrossValidateValues1['fit_time']\n",
    "            r2 = cross_val_score(Model_name, Xroll, y, scoring=\"r2\", cv = cv, n_jobs= -1).mean()\n",
    "            Adj_r2 = 1 - (1-r2*(len(y)-1)/(len(y)-Xroll.shape[1]-1))\n",
    "            mae = cross_val_score(Model_name, Xroll, y, scoring=\"neg_mean_absolute_error\", cv = cv, n_jobs= -1).mean()\n",
    "            msqe = cross_val_score(Model_name, Xroll, y, scoring=\"neg_mean_squared_error\", cv = cv, n_jobs= -1).mean()\n",
    "            # Insert results in dataframe\n",
    "            results_df_regre = results_df_regre.append({'X_shape': X_shape,'y': y_name,'important_features_list': important_features_list, \n",
    "            'important_feature_importances':important_feature_importances, 'Model_used':Model_name,  'feature_Selection_method': feature_Selection_method,          \n",
    "            'Modelcheck_mean': Modelcheck_mean_testscore,'Modelcheck_min': Modelcheck_min_testscore, \n",
    "            'Modelcheck_Q1': test_score_Q1, 'Modelcheck_Q2': test_score_Q2,'Modelcheck_Q3': test_score_Q3,\n",
    "            'Modelcheck_max': Modelcheck_max_testscore,\n",
    "            'roc_F1_score_mean_score': ['Null'], 'roc_auc_mean_score': ['Null'], 'roc_percesion_mean_score': ['Null'],\n",
    "            'roc_sensetivity_mean_score': ['Null'], 'negative_log_likelihood': ['Null'], 'neg_log_loss': ['Null'],\n",
    "            'r2': r2, 'Adj_r2' : Adj_r2, 'neg_mean_absolute_error': mae, 'neg_mean_squared_error':msqe }, ignore_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_regre.to_excel(EXPORT_PATH_Regr, sheet_name='regression results')  \n",
    "results_df_Class.to_excel(EXPORT_PATH_Class, sheet_name='classifier results')  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "536656feb4e5b8426698a6ed7c3ac3e4cf6780e288841fc3960502355480ee44"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
