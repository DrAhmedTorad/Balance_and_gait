{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "# import necessary libraries\n",
    "#----------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "#----------------------------------------------------\n",
    "# Import Classifiers from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#----------------------------------------------------\n",
    "# Import model metrics from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from xgboost import XGBClassifier    \n",
    "#----------------------------------------------------\n",
    "# Import Confusion matrix from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#----------------------------------------------------\n",
    "# Import ROC curve from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "#----------------------------------------------------\n",
    "# Import Model evaluation metrics from libraries\n",
    "#----------------------------------------------------\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#----------------------------------------------------\n",
    "# import data\n",
    "#----------------------------------------------------\n",
    "df = pd.read_excel(r'C:\\Users\\atorad\\Desktop\\Gait_DATA_Y_Range_Media_Mean.xlsx')\n",
    "#----------------------------------------------------\n",
    "# steps variable for iteration\n",
    "#----------------------------------------------------\n",
    "steps = 10\n",
    "#----------------------------------------------------\n",
    "# Algorythm Codes\n",
    "#----------------------------------------------------\n",
    "RandomForestClassifierModel = RandomForestClassifier(criterion = 'gini',n_estimators= steps,max_depth=2,random_state=33) #criterion can be also : entropy \n",
    "LogisticRegressionModel = LogisticRegression(penalty='l2',max_iter = steps, solver='sag',C=1.0,random_state=33)\n",
    "MLPClassifierModel = MLPClassifier(activation='tanh', solver='lbfgs', max_iter = steps, learning_rate='constant', early_stopping= False, alpha=0.0001 ,hidden_layer_sizes=(100, 3),random_state=33)\n",
    "DecisionTreeClassifierModel = DecisionTreeClassifier(criterion='gini',max_depth= steps,random_state=33) \n",
    "SVCModel = SVC(kernel= 'rbf', probability=True, max_iter= steps,C=1.0,gamma='auto')\n",
    "KNNClassifierModel = KNeighborsClassifier(n_neighbors= 5,weights ='uniform', leaf_size = steps, algorithm='auto') \n",
    "GaussianNBModel = GaussianNB()\n",
    "LDAModel = LinearDiscriminantAnalysis(n_components=3,solver='svd',tol=0.0001)\n",
    "GradientBoostingClassifierModel = GradientBoostingClassifier(n_estimators= steps, learning_rate=1.0, max_depth=1, random_state=0) \n",
    "BaggingClassifierModel = BaggingClassifier(base_estimator=SVC(), n_estimators= steps, random_state=0)\n",
    "XGBClassifierModel = XGBClassifier(criterion = 'gini',n_estimators= steps,max_depth=2,random_state=33)\n",
    "#----------------------------------------------------\n",
    "# Create an empty dataframe for results\n",
    "#----------------------------------------------------\n",
    "DF_results = pd.DataFrame(columns=['X_shape','y','important_features_list', 'Model',\n",
    "'MAEValue', 'MSEValue' , 'MdSEValue', 'AccScore', \n",
    "'BAccSc', 'F1Score', 'RecallScore', 'PrecisionScore', 'PrecisionRecallScore', 'PrecisionValue', \n",
    "'RecallValue', 'ThresholdsValue', 'ClassificationReport',\n",
    "'ZeroOneLossValue', 'AUCValue' , 'CM' , \n",
    "'Modelcheck_trainscore' , 'Modelcheck_testscore' , \n",
    "'Modelcheck_fitscore' , 'Modelcheck_Scoretime',\n",
    "'Modelcheck_mean', 'Modelcheck_std', \n",
    "'Modelcheck_min', 'Modelcheck_max', 'Modelcheck_Q1', \n",
    "'Modelcheck_Q2', 'Modelcheck_Q3', 'cross_val_predict_mean', \n",
    "'cross_val_predict_std', 'cross_val_predict_min', 'cross_val_predict_max',\n",
    "'cross_val_predict_Q1', 'cross_val_predict_Q2', 'cross_val_predict_Q3',\n",
    "'cross_val_score_mean', 'cross_val_score_std', 'cross_val_score_min', \n",
    "'cross_val_score_max', 'cross_val_score_Q1', 'cross_val_score_Q2', \n",
    "'cross_val_score_Q3']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:00:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:00:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------\n",
    "# y variable definition\n",
    "#----------------------------------------------------\n",
    "y1 ='Vigor_Fatigue_median'\n",
    "y2 ='Vigor_median'\n",
    "y3 ='Fatigue_Median'\n",
    "y4 ='Vigor_Fatigue_mean'\n",
    "y5 ='Vigor_mean'\n",
    "y6 ='Fatigue_mean'\n",
    "y7 ='Vigor_Fatigue_Range'\n",
    "y8 ='Vigor_Range'\n",
    "y9 ='Fatigue_Range'\n",
    "#----------------------------------------------------\n",
    "# y_name variable for iteration\n",
    "#----------------------------------------------------\n",
    "# y_name_list = [y1, y2, y3, y4, y5, y6, y7, y8, y9] # to be replaced with [y1, y2, y3]\n",
    "y_name_list = [y1] # to be replaced with [y1, y2, y3]\n",
    "#----------------------------------------------------\n",
    "# Creating model list\n",
    "#----------------------------------------------------\n",
    "Model_list = [RandomForestClassifierModel, LogisticRegressionModel, LogisticRegressionModel, MLPClassifierModel,\n",
    "DecisionTreeClassifierModel, SVCModel, KNNClassifierModel, GaussianNBModel, LDAModel, \n",
    "GradientBoostingClassifierModel, BaggingClassifierModel, XGBClassifierModel]\n",
    "#----------------------------------------------------\n",
    "# Creating for loop for y_name_list\n",
    "#----------------------------------------------------\n",
    "for y_name in y_name_list:\n",
    "    y = df[y_name].values\n",
    "    #----------------------------------------------------\n",
    "    # X variable definition\n",
    "    #----------------------------------------------------\n",
    "    X =df.drop(['Vigor_Fatigue_median' ,'Vigor_median' ,'Fatigue_Median' ,'Vigor_Fatigue_mean' ,\n",
    "                'Vigor_mean' ,'Fatigue_mean' ,'Vigor_Fatigue_Range' ,'Vigor_Range' ,\n",
    "                'Fatigue_Range' ,'Vigor_Score' ,'Fatigue_Score' ,'Age' ,'M1/F2' ,'Heightcm' ,\n",
    "                'WeightinKG' ,'CalculatedBMI'],axis=1).values\n",
    "    #----------------------------------------------------\n",
    "    # Cleaning data (Imputation and normalization) of X\n",
    "    #----------------------------------------------------\n",
    "    ImputedModule = IterativeImputer()\n",
    "    ImputedX = ImputedModule.fit(X)\n",
    "    X = ImputedX.transform(X)\n",
    "    transformer = Normalizer(norm='max' ) # max is 1\n",
    "    transformer.fit(X)\n",
    "    X = transformer.transform(X)\n",
    "    #----------------------------------------------------\n",
    "    # Applying the classifier\n",
    "    #----------------------------------------------------\n",
    "    # Get names of X features\n",
    "    X_columns_names = df.columns.drop(['Vigor_Fatigue_median' ,'Vigor_median' ,'Fatigue_Median' ,\n",
    "    'Vigor_Fatigue_mean' , 'Vigor_mean' ,'Fatigue_mean' ,'Vigor_Fatigue_Range' ,'Vigor_Range' ,\n",
    "    'Fatigue_Range' ,'Vigor_Score' ,'Fatigue_Score' ,'Age' ,'M1/F2' ,'Heightcm' ,\n",
    "    'WeightinKG' ,'CalculatedBMI'])\n",
    "    # Test and Train separation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=33, shuffle =False)\n",
    "    # Model identification\n",
    "    RandomForestClassifierModel = RandomForestClassifier(criterion = 'gini',n_estimators= steps,max_depth=2,random_state=33) #criterion can be also : entropy \n",
    "    # Feature selection application\n",
    "    RandomForestClassifierModel.fit(X, y)\n",
    "    # Get importance weights\n",
    "    importance = RandomForestClassifierModel.feature_importances_\n",
    "    # Get features names with their weights\n",
    "    feats = {} \n",
    "    for feature, importance in zip(X_columns_names, importance):\n",
    "        feats[feature] = importance  \n",
    "    importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'importance'})\n",
    "    # Sort features with top 12 important feature at the top\n",
    "    importanc_df = importances.sort_values(by ='importance' ,ascending=False).head(12)\n",
    "    # Create feature names list\n",
    "    important_features_list = importanc_df.index.tolist()\n",
    "    # Create important features dataframe\n",
    "    important_features_df = pd.DataFrame()\n",
    "    # Insert values in dataframe\n",
    "    for K in important_features_list:\n",
    "        important_features_df = important_features_df.append(df[K])\n",
    "    important_features_df = important_features_df.transpose()\n",
    "    XIF = important_features_df.values\n",
    "    #----------------------------------------------------\n",
    "    # Cleaning data (Imputation and normalization) of XIF\n",
    "    #----------------------------------------------------\n",
    "    ImputedModule = IterativeImputer()\n",
    "    ImputedX = ImputedModule.fit(XIF)\n",
    "    XIF = ImputedX.transform(XIF)\n",
    "    transformer = Normalizer(norm='max' ) # max is 1\n",
    "    transformer.fit(XIF)\n",
    "    XIF = transformer.transform(XIF)\n",
    "    # Make X list\n",
    "    X_list = [X, XIF]\n",
    "    for Xroll in X_list:\n",
    "        S = Xroll.shape\n",
    "        #Splitting data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(Xroll, y, test_size=0.10, random_state=33, shuffle =True)\n",
    "        for i in Model_list:\n",
    "            # Model train\n",
    "            i.fit(X_train, y_train)\n",
    "            y_pred = i.predict(X_test)\n",
    "            y_pred_prob = i.predict_proba(X_test)\n",
    "            # metrics\n",
    "            MAEValue = mean_absolute_error(y_test, y_pred, multioutput='uniform_average') # it can be raw_values\n",
    "            MSEValue = mean_squared_error(y_test, y_pred, multioutput='uniform_average') # it can be raw_values\n",
    "            MdSEValue = median_absolute_error(y_test, y_pred)\n",
    "            AccScore = accuracy_score(y_test, y_pred, normalize=True)\n",
    "            BAccSc = balanced_accuracy_score(y_test,y_pred) \n",
    "            F1Score = f1_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples\n",
    "            RecallScore = recall_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples\n",
    "            PrecisionScore = precision_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples\n",
    "            PrecisionRecallScore = precision_recall_fscore_support(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples\n",
    "            PrecisionValue, RecallValue, ThresholdsValue = precision_recall_curve(y_test,y_pred, pos_label=1, sample_weight=None)\n",
    "            ClassificationReport = classification_report(y_test,y_pred)\n",
    "            ZeroOneLossValue = zero_one_loss(y_test,y_pred,normalize=False) \n",
    "            # Confusion matrix\n",
    "            CM = confusion_matrix(y_test, y_pred)\n",
    "            '''sns.heatmap(CM, center = True)\n",
    "            plt.show()'''\n",
    "            # ROC curve\n",
    "            fprValue, tprValue, thresholdsValue = roc_curve(y_test,y_pred, pos_label=1)\n",
    "            fprValue2, tprValue2, thresholdsValue2 = roc_curve(y_test,y_pred, pos_label=1)\n",
    "            AUCValue = auc(fprValue2, tprValue2)\n",
    "            SelectedModel = i\n",
    "            #  Model Check\n",
    "            CrossValidateValues1 = cross_validate(SelectedModel,Xroll,y,cv=4,return_train_score = True)\n",
    "            CrossValidateValues2 = cross_validate(SelectedModel,Xroll,y,cv=4,scoring=('r2','neg_mean_squared_error'))\n",
    "            Modelcheck_trainscore = CrossValidateValues1['train_score']\n",
    "            Modelcheck_testscore = CrossValidateValues1['test_score'] \n",
    "            Modelcheck_fitscore = CrossValidateValues1['fit_time']\n",
    "            Modelcheck_Scoretime = CrossValidateValues1['score_time']\n",
    "            # Model check evaluation\n",
    "            Modelcheck_mean = np.mean(CrossValidateValues1['test_score']) \n",
    "            Modelcheck_std = np.std(CrossValidateValues1['test_score']) \n",
    "            Modelcheck_min = CrossValidateValues1['test_score'].min()\n",
    "            Modelcheck_max = CrossValidateValues1['test_score'].max()\n",
    "            Modelcheck_Q1 = np.quantile(CrossValidateValues1['test_score'], .25)\n",
    "            Modelcheck_Q2 = np.quantile(CrossValidateValues1['test_score'], .50)\n",
    "            Modelcheck_Q3 = np.quantile(CrossValidateValues1['test_score'], .75)\n",
    "            # cross_val_predict\n",
    "            CrossValidatePredictionTrain = cross_val_predict(SelectedModel, Xroll, y, cv=4)\n",
    "            CrossValidatePredictionTest = cross_val_predict(SelectedModel, Xroll, y, cv=4)\n",
    "            # cross_val_predict values\n",
    "            cross_val_predict_mean = CrossValidatePredictionTest.mean()\n",
    "            cross_val_predict_std = CrossValidatePredictionTest.std()\n",
    "            cross_val_predict_min = CrossValidatePredictionTest.min()\n",
    "            cross_val_predict_max = CrossValidatePredictionTest.max()\n",
    "            cross_val_predict_Q1 = np.quantile(CrossValidatePredictionTest, .25)\n",
    "            cross_val_predict_Q2 = np.quantile(CrossValidatePredictionTest, .50)\n",
    "            cross_val_predict_Q3 = np.quantile(CrossValidatePredictionTest, .75)\n",
    "            # cross_val_score  \n",
    "            CrossValidateScoreTrain = cross_val_score(SelectedModel, Xroll, y, cv=4)\n",
    "            CrossValidateScoreTest = cross_val_score(SelectedModel, Xroll, y, cv=4)\n",
    "            # cross_val_score evaluation\n",
    "            cross_val_score_mean = CrossValidateScoreTest.mean()\n",
    "            cross_val_score_std = CrossValidateScoreTest.std()\n",
    "            cross_val_score_min = CrossValidateScoreTest.min()\n",
    "            cross_val_score_max = Pipeline_mean = CrossValidateScoreTest.max()\n",
    "            cross_val_score_Q1 = np.quantile(CrossValidateScoreTest, .25)\n",
    "            cross_val_score_Q2 = np.quantile(CrossValidateScoreTest, .50)\n",
    "            cross_val_score_Q3 = np.quantile(CrossValidateScoreTest, .75)\n",
    "            # Insert results in dataframe\n",
    "            DF_results = DF_results.append({'X_shape': S,'y': y_name,'important_features_list': important_features_list, 'Model': str(i),\n",
    "            'MAEValue': MAEValue, 'MSEValue': MSEValue , 'MdSEValue': MdSEValue, 'AccScore': MdSEValue, \n",
    "            'BAccSc': BAccSc, 'F1Score': F1Score, 'RecallScore': RecallScore, 'PrecisionScore': PrecisionScore, \n",
    "            'PrecisionRecallScore': PrecisionRecallScore, 'PrecisionValue': PrecisionValue, \n",
    "            'RecallValue': RecallValue, 'ThresholdsValue':ThresholdsValue, 'ClassificationReport':ClassificationReport,\n",
    "            'ZeroOneLossValue': ZeroOneLossValue, 'AUCValue': AUCValue , 'CM' : CM, \n",
    "            'Modelcheck_trainscore'  : Modelcheck_trainscore , 'Modelcheck_testscore' : Modelcheck_testscore , \n",
    "            'Modelcheck_fitscore' : Modelcheck_fitscore, 'Modelcheck_Scoretime' : Modelcheck_Scoretime,            \n",
    "            'Modelcheck_mean': Modelcheck_mean, 'Modelcheck_std': Modelcheck_std, \n",
    "            'Modelcheck_min': Modelcheck_min, 'Modelcheck_max': Modelcheck_max, 'Modelcheck_Q1': Modelcheck_Q1, \n",
    "            'Modelcheck_Q2': Modelcheck_Q2, 'Modelcheck_Q3': Modelcheck_Q3, 'cross_val_predict_mean': cross_val_predict_mean, \n",
    "            'cross_val_predict_std': cross_val_predict_std, 'cross_val_predict_min': cross_val_predict_min, 'cross_val_predict_max': cross_val_predict_max,\n",
    "            'cross_val_predict_Q1': cross_val_predict_Q1, 'cross_val_predict_Q2': cross_val_predict_Q2, 'cross_val_predict_Q3': cross_val_predict_Q3,\n",
    "            'cross_val_score_mean': cross_val_score_mean, 'cross_val_score_std': cross_val_score_std, 'cross_val_score_min': cross_val_score_min, \n",
    "            'cross_val_score_max': cross_val_score_max, 'cross_val_score_Q1': cross_val_score_Q1, 'cross_val_score_Q2': cross_val_score_Q2, \n",
    "            'cross_val_score_Q3': cross_val_score_Q3}, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_shape</th>\n",
       "      <th>y</th>\n",
       "      <th>important_features_list</th>\n",
       "      <th>Model</th>\n",
       "      <th>MAEValue</th>\n",
       "      <th>MSEValue</th>\n",
       "      <th>MdSEValue</th>\n",
       "      <th>AccScore</th>\n",
       "      <th>BAccSc</th>\n",
       "      <th>F1Score</th>\n",
       "      <th>...</th>\n",
       "      <th>cross_val_predict_Q1</th>\n",
       "      <th>cross_val_predict_Q2</th>\n",
       "      <th>cross_val_predict_Q3</th>\n",
       "      <th>cross_val_score_mean</th>\n",
       "      <th>cross_val_score_std</th>\n",
       "      <th>cross_val_score_min</th>\n",
       "      <th>cross_val_score_max</th>\n",
       "      <th>cross_val_score_Q1</th>\n",
       "      <th>cross_val_score_Q2</th>\n",
       "      <th>cross_val_score_Q3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.224376</td>\n",
       "      <td>0.046799</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.213235</td>\n",
       "      <td>0.238859</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>LogisticRegression(max_iter=10, random_state=3...</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.299020</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.268717</td>\n",
       "      <td>0.303030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>LogisticRegression(max_iter=10, random_state=3...</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.299020</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.268717</td>\n",
       "      <td>0.303030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>MLPClassifier(activation='tanh', hidden_layer_...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.214286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.245766</td>\n",
       "      <td>0.041222</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.253565</td>\n",
       "      <td>0.272059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=10, random_st...</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.642857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.305927</td>\n",
       "      <td>0.052074</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.286096</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.322861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>SVC(gamma='auto', max_iter=10, probability=True)</td>\n",
       "      <td>1.357143</td>\n",
       "      <td>2.642857</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.238859</td>\n",
       "      <td>0.030073</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.210561</td>\n",
       "      <td>0.238414</td>\n",
       "      <td>0.266711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>KNeighborsClassifier(leaf_size=10)</td>\n",
       "      <td>1.214286</td>\n",
       "      <td>2.357143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.276292</td>\n",
       "      <td>0.015781</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.268717</td>\n",
       "      <td>0.280303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.335116</td>\n",
       "      <td>0.074387</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.317736</td>\n",
       "      <td>0.358289</td>\n",
       "      <td>0.375668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=3)</td>\n",
       "      <td>1.357143</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.276738</td>\n",
       "      <td>0.066938</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.248663</td>\n",
       "      <td>0.283422</td>\n",
       "      <td>0.311497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.313280</td>\n",
       "      <td>0.031571</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.288770</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.338235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>(SVC(random_state=2087557356), SVC(random_stat...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.298128</td>\n",
       "      <td>0.039203</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.281194</td>\n",
       "      <td>0.298574</td>\n",
       "      <td>0.315508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.357143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.320633</td>\n",
       "      <td>0.029616</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.310829</td>\n",
       "      <td>0.328431</td>\n",
       "      <td>0.338235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=2, max_featu...</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.642857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.245989</td>\n",
       "      <td>0.042613</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.221925</td>\n",
       "      <td>0.254011</td>\n",
       "      <td>0.278075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>LogisticRegression(max_iter=10, random_state=3...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.313503</td>\n",
       "      <td>0.015646</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.300802</td>\n",
       "      <td>0.313280</td>\n",
       "      <td>0.325980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>LogisticRegression(max_iter=10, random_state=3...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.313503</td>\n",
       "      <td>0.015646</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.300802</td>\n",
       "      <td>0.313280</td>\n",
       "      <td>0.325980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>MLPClassifier(activation='tanh', hidden_layer_...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.275624</td>\n",
       "      <td>0.055199</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.266043</td>\n",
       "      <td>0.298574</td>\n",
       "      <td>0.308155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=10, random_st...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.246435</td>\n",
       "      <td>0.053733</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.203209</td>\n",
       "      <td>0.253119</td>\n",
       "      <td>0.296346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>SVC(gamma='auto', max_iter=10, probability=True)</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.238859</td>\n",
       "      <td>0.059828</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.180481</td>\n",
       "      <td>0.237968</td>\n",
       "      <td>0.296346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>KNeighborsClassifier(leaf_size=10)</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.642857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.321747</td>\n",
       "      <td>0.078251</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.283868</td>\n",
       "      <td>0.340909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.276738</td>\n",
       "      <td>0.075664</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264037</td>\n",
       "      <td>0.313280</td>\n",
       "      <td>0.325980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=3)</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.297906</td>\n",
       "      <td>0.052609</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.280303</td>\n",
       "      <td>0.313280</td>\n",
       "      <td>0.330882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.305704</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.288770</td>\n",
       "      <td>0.298574</td>\n",
       "      <td>0.315508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>(SVC(random_state=2087557356), SVC(random_stat...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.313503</td>\n",
       "      <td>0.015646</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.300802</td>\n",
       "      <td>0.313280</td>\n",
       "      <td>0.325980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.314171</td>\n",
       "      <td>0.082454</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.343137</td>\n",
       "      <td>0.363191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>RandomForestClassifier(max_depth=2, n_estimato...</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.224376</td>\n",
       "      <td>0.046799</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.213235</td>\n",
       "      <td>0.238859</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>LogisticRegression(max_iter=10, random_state=3...</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.299020</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.268717</td>\n",
       "      <td>0.303030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>LogisticRegression(max_iter=10, random_state=3...</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.299020</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.268717</td>\n",
       "      <td>0.303030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>MLPClassifier(activation='tanh', hidden_layer_...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.214286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.245766</td>\n",
       "      <td>0.041222</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.253565</td>\n",
       "      <td>0.272059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=10, random_st...</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.642857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.305927</td>\n",
       "      <td>0.052074</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.286096</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.322861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>SVC(gamma='auto', max_iter=10, probability=True)</td>\n",
       "      <td>1.357143</td>\n",
       "      <td>2.642857</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.238859</td>\n",
       "      <td>0.030073</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.210561</td>\n",
       "      <td>0.238414</td>\n",
       "      <td>0.266711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>KNeighborsClassifier(leaf_size=10)</td>\n",
       "      <td>1.214286</td>\n",
       "      <td>2.357143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.276292</td>\n",
       "      <td>0.015781</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.268717</td>\n",
       "      <td>0.280303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.335116</td>\n",
       "      <td>0.074387</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.317736</td>\n",
       "      <td>0.358289</td>\n",
       "      <td>0.375668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=3)</td>\n",
       "      <td>1.357143</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.276738</td>\n",
       "      <td>0.066938</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.248663</td>\n",
       "      <td>0.283422</td>\n",
       "      <td>0.311497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>GradientBoostingClassifier(learning_rate=1.0, ...</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.313280</td>\n",
       "      <td>0.031571</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.288770</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.338235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>BaggingClassifier(base_estimator=SVC(), random...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.298128</td>\n",
       "      <td>0.039203</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.281194</td>\n",
       "      <td>0.298574</td>\n",
       "      <td>0.315508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(134, 168)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.357143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.320633</td>\n",
       "      <td>0.029616</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.310829</td>\n",
       "      <td>0.328431</td>\n",
       "      <td>0.338235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>RandomForestClassifier(max_depth=2, n_estimato...</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.642857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.245989</td>\n",
       "      <td>0.042613</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.221925</td>\n",
       "      <td>0.254011</td>\n",
       "      <td>0.278075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>LogisticRegression(max_iter=10, random_state=3...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.313503</td>\n",
       "      <td>0.015646</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.300802</td>\n",
       "      <td>0.313280</td>\n",
       "      <td>0.325980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>LogisticRegression(max_iter=10, random_state=3...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.313503</td>\n",
       "      <td>0.015646</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.300802</td>\n",
       "      <td>0.313280</td>\n",
       "      <td>0.325980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>MLPClassifier(activation='tanh', hidden_layer_...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.275624</td>\n",
       "      <td>0.055199</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.266043</td>\n",
       "      <td>0.298574</td>\n",
       "      <td>0.308155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=10, random_st...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.246435</td>\n",
       "      <td>0.053733</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.203209</td>\n",
       "      <td>0.253119</td>\n",
       "      <td>0.296346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>SVC(gamma='auto', max_iter=10, probability=True)</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.238859</td>\n",
       "      <td>0.059828</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.180481</td>\n",
       "      <td>0.237968</td>\n",
       "      <td>0.296346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>KNeighborsClassifier(leaf_size=10)</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.642857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.321747</td>\n",
       "      <td>0.078251</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.283868</td>\n",
       "      <td>0.340909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.276738</td>\n",
       "      <td>0.075664</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264037</td>\n",
       "      <td>0.313280</td>\n",
       "      <td>0.325980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=3)</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.297906</td>\n",
       "      <td>0.052609</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.280303</td>\n",
       "      <td>0.313280</td>\n",
       "      <td>0.330882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>GradientBoostingClassifier(learning_rate=1.0, ...</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.305704</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.288770</td>\n",
       "      <td>0.298574</td>\n",
       "      <td>0.315508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>BaggingClassifier(base_estimator=SVC(), random...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.313503</td>\n",
       "      <td>0.015646</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.300802</td>\n",
       "      <td>0.313280</td>\n",
       "      <td>0.325980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(134, 12)</td>\n",
       "      <td>Vigor_Fatigue_median</td>\n",
       "      <td>[GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.314171</td>\n",
       "      <td>0.082454</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.343137</td>\n",
       "      <td>0.363191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X_shape                     y  \\\n",
       "0   (134, 168)  Vigor_Fatigue_median   \n",
       "1   (134, 168)  Vigor_Fatigue_median   \n",
       "2   (134, 168)  Vigor_Fatigue_median   \n",
       "3   (134, 168)  Vigor_Fatigue_median   \n",
       "4   (134, 168)  Vigor_Fatigue_median   \n",
       "5   (134, 168)  Vigor_Fatigue_median   \n",
       "6   (134, 168)  Vigor_Fatigue_median   \n",
       "7   (134, 168)  Vigor_Fatigue_median   \n",
       "8   (134, 168)  Vigor_Fatigue_median   \n",
       "9   (134, 168)  Vigor_Fatigue_median   \n",
       "10  (134, 168)  Vigor_Fatigue_median   \n",
       "11  (134, 168)  Vigor_Fatigue_median   \n",
       "12   (134, 12)  Vigor_Fatigue_median   \n",
       "13   (134, 12)  Vigor_Fatigue_median   \n",
       "14   (134, 12)  Vigor_Fatigue_median   \n",
       "15   (134, 12)  Vigor_Fatigue_median   \n",
       "16   (134, 12)  Vigor_Fatigue_median   \n",
       "17   (134, 12)  Vigor_Fatigue_median   \n",
       "18   (134, 12)  Vigor_Fatigue_median   \n",
       "19   (134, 12)  Vigor_Fatigue_median   \n",
       "20   (134, 12)  Vigor_Fatigue_median   \n",
       "21   (134, 12)  Vigor_Fatigue_median   \n",
       "22   (134, 12)  Vigor_Fatigue_median   \n",
       "23   (134, 12)  Vigor_Fatigue_median   \n",
       "24  (134, 168)  Vigor_Fatigue_median   \n",
       "25  (134, 168)  Vigor_Fatigue_median   \n",
       "26  (134, 168)  Vigor_Fatigue_median   \n",
       "27  (134, 168)  Vigor_Fatigue_median   \n",
       "28  (134, 168)  Vigor_Fatigue_median   \n",
       "29  (134, 168)  Vigor_Fatigue_median   \n",
       "30  (134, 168)  Vigor_Fatigue_median   \n",
       "31  (134, 168)  Vigor_Fatigue_median   \n",
       "32  (134, 168)  Vigor_Fatigue_median   \n",
       "33  (134, 168)  Vigor_Fatigue_median   \n",
       "34  (134, 168)  Vigor_Fatigue_median   \n",
       "35  (134, 168)  Vigor_Fatigue_median   \n",
       "36   (134, 12)  Vigor_Fatigue_median   \n",
       "37   (134, 12)  Vigor_Fatigue_median   \n",
       "38   (134, 12)  Vigor_Fatigue_median   \n",
       "39   (134, 12)  Vigor_Fatigue_median   \n",
       "40   (134, 12)  Vigor_Fatigue_median   \n",
       "41   (134, 12)  Vigor_Fatigue_median   \n",
       "42   (134, 12)  Vigor_Fatigue_median   \n",
       "43   (134, 12)  Vigor_Fatigue_median   \n",
       "44   (134, 12)  Vigor_Fatigue_median   \n",
       "45   (134, 12)  Vigor_Fatigue_median   \n",
       "46   (134, 12)  Vigor_Fatigue_median   \n",
       "47   (134, 12)  Vigor_Fatigue_median   \n",
       "\n",
       "                              important_features_list  \\\n",
       "0   [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "1   [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "2   [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "3   [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "4   [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "5   [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "6   [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "7   [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "8   [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "9   [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "10  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "11  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "12  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "13  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "14  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "15  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "16  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "17  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "18  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "19  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "20  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "21  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "22  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "23  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "24  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "25  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "26  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "27  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "28  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "29  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "30  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "31  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "32  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "33  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "34  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "35  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "36  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "37  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "38  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "39  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "40  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "41  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "42  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "43  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "44  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "45  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "46  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "47  [GaitLowerLimbToeOutAngleRdegreesstd, GaitJoin...   \n",
       "\n",
       "                                                Model  MAEValue  MSEValue  \\\n",
       "0   (DecisionTreeClassifier(max_depth=2, max_featu...  1.071429  1.785714   \n",
       "1   LogisticRegression(max_iter=10, random_state=3...  0.857143  1.142857   \n",
       "2   LogisticRegression(max_iter=10, random_state=3...  0.857143  1.142857   \n",
       "3   MLPClassifier(activation='tanh', hidden_layer_...  0.928571  1.214286   \n",
       "4   DecisionTreeClassifier(max_depth=10, random_st...  1.071429  1.642857   \n",
       "5    SVC(gamma='auto', max_iter=10, probability=True)  1.357143  2.642857   \n",
       "6                  KNeighborsClassifier(leaf_size=10)  1.214286  2.357143   \n",
       "7                                        GaussianNB()  0.857143  1.142857   \n",
       "8          LinearDiscriminantAnalysis(n_components=3)  1.357143  2.500000   \n",
       "9   ([DecisionTreeRegressor(criterion='friedman_ms...  1.142857  2.285714   \n",
       "10  (SVC(random_state=2087557356), SVC(random_stat...  1.000000  1.571429   \n",
       "11  XGBClassifier(base_score=0.5, booster='gbtree'...  0.928571  1.357143   \n",
       "12  (DecisionTreeClassifier(max_depth=2, max_featu...  1.071429  1.642857   \n",
       "13  LogisticRegression(max_iter=10, random_state=3...  1.000000  1.571429   \n",
       "14  LogisticRegression(max_iter=10, random_state=3...  1.000000  1.571429   \n",
       "15  MLPClassifier(activation='tanh', hidden_layer_...  1.000000  1.571429   \n",
       "16  DecisionTreeClassifier(max_depth=10, random_st...  1.000000  1.714286   \n",
       "17   SVC(gamma='auto', max_iter=10, probability=True)  1.714286  4.000000   \n",
       "18                 KNeighborsClassifier(leaf_size=10)  1.071429  1.642857   \n",
       "19                                       GaussianNB()  1.000000  1.428571   \n",
       "20         LinearDiscriminantAnalysis(n_components=3)  1.285714  2.000000   \n",
       "21  ([DecisionTreeRegressor(criterion='friedman_ms...  1.142857  2.000000   \n",
       "22  (SVC(random_state=2087557356), SVC(random_stat...  1.000000  1.571429   \n",
       "23  XGBClassifier(base_score=0.5, booster='gbtree'...  1.071429  1.500000   \n",
       "24  RandomForestClassifier(max_depth=2, n_estimato...  1.071429  1.785714   \n",
       "25  LogisticRegression(max_iter=10, random_state=3...  0.857143  1.142857   \n",
       "26  LogisticRegression(max_iter=10, random_state=3...  0.857143  1.142857   \n",
       "27  MLPClassifier(activation='tanh', hidden_layer_...  0.928571  1.214286   \n",
       "28  DecisionTreeClassifier(max_depth=10, random_st...  1.071429  1.642857   \n",
       "29   SVC(gamma='auto', max_iter=10, probability=True)  1.357143  2.642857   \n",
       "30                 KNeighborsClassifier(leaf_size=10)  1.214286  2.357143   \n",
       "31                                       GaussianNB()  0.857143  1.142857   \n",
       "32         LinearDiscriminantAnalysis(n_components=3)  1.357143  2.500000   \n",
       "33  GradientBoostingClassifier(learning_rate=1.0, ...  1.142857  2.285714   \n",
       "34  BaggingClassifier(base_estimator=SVC(), random...  1.000000  1.571429   \n",
       "35  XGBClassifier(base_score=0.5, booster='gbtree'...  0.928571  1.357143   \n",
       "36  RandomForestClassifier(max_depth=2, n_estimato...  1.071429  1.642857   \n",
       "37  LogisticRegression(max_iter=10, random_state=3...  1.000000  1.571429   \n",
       "38  LogisticRegression(max_iter=10, random_state=3...  1.000000  1.571429   \n",
       "39  MLPClassifier(activation='tanh', hidden_layer_...  1.000000  1.571429   \n",
       "40  DecisionTreeClassifier(max_depth=10, random_st...  1.000000  1.714286   \n",
       "41   SVC(gamma='auto', max_iter=10, probability=True)  1.714286  4.000000   \n",
       "42                 KNeighborsClassifier(leaf_size=10)  1.071429  1.642857   \n",
       "43                                       GaussianNB()  1.000000  1.428571   \n",
       "44         LinearDiscriminantAnalysis(n_components=3)  1.285714  2.000000   \n",
       "45  GradientBoostingClassifier(learning_rate=1.0, ...  1.142857  2.000000   \n",
       "46  BaggingClassifier(base_estimator=SVC(), random...  1.000000  1.571429   \n",
       "47  XGBClassifier(base_score=0.5, booster='gbtree'...  1.071429  1.500000   \n",
       "\n",
       "    MdSEValue  AccScore  BAccSc   F1Score  ...  cross_val_predict_Q1  \\\n",
       "0         1.0       1.0  0.2500  0.285714  ...                  2.00   \n",
       "1         1.0       1.0  0.2500  0.285714  ...                  2.00   \n",
       "2         1.0       1.0  0.2500  0.285714  ...                  2.00   \n",
       "3         1.0       1.0  0.1875  0.214286  ...                  1.00   \n",
       "4         1.0       1.0  0.1875  0.214286  ...                  1.25   \n",
       "5         1.5       1.5  0.2500  0.214286  ...                  1.00   \n",
       "6         1.0       1.0  0.2500  0.285714  ...                  1.00   \n",
       "7         1.0       1.0  0.3750  0.285714  ...                  2.00   \n",
       "8         1.0       1.0  0.1250  0.142857  ...                  1.00   \n",
       "9         1.0       1.0  0.2500  0.285714  ...                  2.00   \n",
       "10        1.0       1.0  0.2500  0.285714  ...                  2.00   \n",
       "11        1.0       1.0  0.2500  0.285714  ...                  2.00   \n",
       "12        1.0       1.0  0.1875  0.214286  ...                  2.00   \n",
       "13        1.0       1.0  0.2500  0.285714  ...                  3.00   \n",
       "14        1.0       1.0  0.2500  0.285714  ...                  3.00   \n",
       "15        1.0       1.0  0.2500  0.285714  ...                  3.00   \n",
       "16        1.0       1.0  0.2500  0.285714  ...                  1.25   \n",
       "17        2.0       2.0  0.1250  0.142857  ...                  3.00   \n",
       "18        1.0       1.0  0.1875  0.214286  ...                  1.00   \n",
       "19        1.0       1.0  0.1875  0.214286  ...                  2.00   \n",
       "20        1.0       1.0  0.0625  0.071429  ...                  2.00   \n",
       "21        1.0       1.0  0.1875  0.214286  ...                  1.00   \n",
       "22        1.0       1.0  0.2500  0.285714  ...                  3.00   \n",
       "23        1.0       1.0  0.1250  0.142857  ...                  1.00   \n",
       "24        1.0       1.0  0.2500  0.285714  ...                  2.00   \n",
       "25        1.0       1.0  0.2500  0.285714  ...                  2.00   \n",
       "26        1.0       1.0  0.2500  0.285714  ...                  2.00   \n",
       "27        1.0       1.0  0.1875  0.214286  ...                  1.00   \n",
       "28        1.0       1.0  0.1875  0.214286  ...                  1.25   \n",
       "29        1.5       1.5  0.2500  0.214286  ...                  1.00   \n",
       "30        1.0       1.0  0.2500  0.285714  ...                  1.00   \n",
       "31        1.0       1.0  0.3750  0.285714  ...                  2.00   \n",
       "32        1.0       1.0  0.1250  0.142857  ...                  1.00   \n",
       "33        1.0       1.0  0.2500  0.285714  ...                  2.00   \n",
       "34        1.0       1.0  0.2500  0.285714  ...                  2.00   \n",
       "35        1.0       1.0  0.2500  0.285714  ...                  2.00   \n",
       "36        1.0       1.0  0.1875  0.214286  ...                  2.00   \n",
       "37        1.0       1.0  0.2500  0.285714  ...                  3.00   \n",
       "38        1.0       1.0  0.2500  0.285714  ...                  3.00   \n",
       "39        1.0       1.0  0.2500  0.285714  ...                  3.00   \n",
       "40        1.0       1.0  0.2500  0.285714  ...                  1.25   \n",
       "41        2.0       2.0  0.1250  0.142857  ...                  3.00   \n",
       "42        1.0       1.0  0.1875  0.214286  ...                  1.00   \n",
       "43        1.0       1.0  0.1875  0.214286  ...                  2.00   \n",
       "44        1.0       1.0  0.0625  0.071429  ...                  2.00   \n",
       "45        1.0       1.0  0.1875  0.214286  ...                  1.00   \n",
       "46        1.0       1.0  0.2500  0.285714  ...                  3.00   \n",
       "47        1.0       1.0  0.1250  0.142857  ...                  1.00   \n",
       "\n",
       "    cross_val_predict_Q2 cross_val_predict_Q3 cross_val_score_mean  \\\n",
       "0                    3.0                 3.00             0.224376   \n",
       "1                    3.0                 3.00             0.299020   \n",
       "2                    3.0                 3.00             0.299020   \n",
       "3                    2.0                 3.00             0.245766   \n",
       "4                    2.5                 3.00             0.305927   \n",
       "5                    3.0                 4.00             0.238859   \n",
       "6                    2.0                 3.00             0.276292   \n",
       "7                    2.0                 3.75             0.335116   \n",
       "8                    3.0                 3.00             0.276738   \n",
       "9                    2.0                 3.00             0.313280   \n",
       "10                   3.0                 3.00             0.298128   \n",
       "11                   3.0                 3.00             0.320633   \n",
       "12                   3.0                 3.00             0.245989   \n",
       "13                   3.0                 3.00             0.313503   \n",
       "14                   3.0                 3.00             0.313503   \n",
       "15                   3.0                 3.00             0.275624   \n",
       "16                   2.0                 3.00             0.246435   \n",
       "17                   4.0                 4.00             0.238859   \n",
       "18                   2.0                 3.00             0.321747   \n",
       "19                   3.0                 3.00             0.276738   \n",
       "20                   3.0                 3.00             0.297906   \n",
       "21                   2.0                 3.00             0.305704   \n",
       "22                   3.0                 3.00             0.313503   \n",
       "23                   3.0                 3.00             0.314171   \n",
       "24                   3.0                 3.00             0.224376   \n",
       "25                   3.0                 3.00             0.299020   \n",
       "26                   3.0                 3.00             0.299020   \n",
       "27                   2.0                 3.00             0.245766   \n",
       "28                   2.5                 3.00             0.305927   \n",
       "29                   3.0                 4.00             0.238859   \n",
       "30                   2.0                 3.00             0.276292   \n",
       "31                   2.0                 3.75             0.335116   \n",
       "32                   3.0                 3.00             0.276738   \n",
       "33                   2.0                 3.00             0.313280   \n",
       "34                   3.0                 3.00             0.298128   \n",
       "35                   3.0                 3.00             0.320633   \n",
       "36                   3.0                 3.00             0.245989   \n",
       "37                   3.0                 3.00             0.313503   \n",
       "38                   3.0                 3.00             0.313503   \n",
       "39                   3.0                 3.00             0.275624   \n",
       "40                   2.0                 3.00             0.246435   \n",
       "41                   4.0                 4.00             0.238859   \n",
       "42                   2.0                 3.00             0.321747   \n",
       "43                   3.0                 3.00             0.276738   \n",
       "44                   3.0                 3.00             0.297906   \n",
       "45                   2.0                 3.00             0.305704   \n",
       "46                   3.0                 3.00             0.313503   \n",
       "47                   3.0                 3.00             0.314171   \n",
       "\n",
       "   cross_val_score_std cross_val_score_min cross_val_score_max  \\\n",
       "0             0.046799            0.147059            0.272727   \n",
       "1             0.054900            0.264706            0.393939   \n",
       "2             0.054900            0.264706            0.393939   \n",
       "3             0.041222            0.181818            0.294118   \n",
       "4             0.052074            0.235294            0.382353   \n",
       "5             0.030073            0.205882            0.272727   \n",
       "6             0.015781            0.264706            0.303030   \n",
       "7             0.074387            0.212121            0.411765   \n",
       "8             0.066938            0.176471            0.363636   \n",
       "9             0.031571            0.272727            0.352941   \n",
       "10            0.039203            0.242424            0.352941   \n",
       "11            0.029616            0.272727            0.352941   \n",
       "12            0.042613            0.181818            0.294118   \n",
       "13            0.015646            0.294118            0.333333   \n",
       "14            0.015646            0.294118            0.333333   \n",
       "15            0.055199            0.181818            0.323529   \n",
       "16            0.053733            0.176471            0.303030   \n",
       "17            0.059828            0.176471            0.303030   \n",
       "18            0.078251            0.264706            0.454545   \n",
       "19            0.075664            0.147059            0.333333   \n",
       "20            0.052609            0.212121            0.352941   \n",
       "21            0.029412            0.272727            0.352941   \n",
       "22            0.015646            0.294118            0.333333   \n",
       "23            0.082454            0.176471            0.393939   \n",
       "24            0.046799            0.147059            0.272727   \n",
       "25            0.054900            0.264706            0.393939   \n",
       "26            0.054900            0.264706            0.393939   \n",
       "27            0.041222            0.181818            0.294118   \n",
       "28            0.052074            0.235294            0.382353   \n",
       "29            0.030073            0.205882            0.272727   \n",
       "30            0.015781            0.264706            0.303030   \n",
       "31            0.074387            0.212121            0.411765   \n",
       "32            0.066938            0.176471            0.363636   \n",
       "33            0.031571            0.272727            0.352941   \n",
       "34            0.039203            0.242424            0.352941   \n",
       "35            0.029616            0.272727            0.352941   \n",
       "36            0.042613            0.181818            0.294118   \n",
       "37            0.015646            0.294118            0.333333   \n",
       "38            0.015646            0.294118            0.333333   \n",
       "39            0.055199            0.181818            0.323529   \n",
       "40            0.053733            0.176471            0.303030   \n",
       "41            0.059828            0.176471            0.303030   \n",
       "42            0.078251            0.264706            0.454545   \n",
       "43            0.075664            0.147059            0.333333   \n",
       "44            0.052609            0.212121            0.352941   \n",
       "45            0.029412            0.272727            0.352941   \n",
       "46            0.015646            0.294118            0.333333   \n",
       "47            0.082454            0.176471            0.393939   \n",
       "\n",
       "   cross_val_score_Q1  cross_val_score_Q2 cross_val_score_Q3  \n",
       "0            0.213235            0.238859           0.250000  \n",
       "1            0.264706            0.268717           0.303030  \n",
       "2            0.264706            0.268717           0.303030  \n",
       "3            0.227273            0.253565           0.272059  \n",
       "4            0.286096            0.303030           0.322861  \n",
       "5            0.210561            0.238414           0.266711  \n",
       "6            0.264706            0.268717           0.280303  \n",
       "7            0.317736            0.358289           0.375668  \n",
       "8            0.248663            0.283422           0.311497  \n",
       "9            0.288770            0.313725           0.338235  \n",
       "10           0.281194            0.298574           0.315508  \n",
       "11           0.310829            0.328431           0.338235  \n",
       "12           0.221925            0.254011           0.278075  \n",
       "13           0.300802            0.313280           0.325980  \n",
       "14           0.300802            0.313280           0.325980  \n",
       "15           0.266043            0.298574           0.308155  \n",
       "16           0.203209            0.253119           0.296346  \n",
       "17           0.180481            0.237968           0.296346  \n",
       "18           0.264706            0.283868           0.340909  \n",
       "19           0.264037            0.313280           0.325980  \n",
       "20           0.280303            0.313280           0.330882  \n",
       "21           0.288770            0.298574           0.315508  \n",
       "22           0.300802            0.313280           0.325980  \n",
       "23           0.294118            0.343137           0.363191  \n",
       "24           0.213235            0.238859           0.250000  \n",
       "25           0.264706            0.268717           0.303030  \n",
       "26           0.264706            0.268717           0.303030  \n",
       "27           0.227273            0.253565           0.272059  \n",
       "28           0.286096            0.303030           0.322861  \n",
       "29           0.210561            0.238414           0.266711  \n",
       "30           0.264706            0.268717           0.280303  \n",
       "31           0.317736            0.358289           0.375668  \n",
       "32           0.248663            0.283422           0.311497  \n",
       "33           0.288770            0.313725           0.338235  \n",
       "34           0.281194            0.298574           0.315508  \n",
       "35           0.310829            0.328431           0.338235  \n",
       "36           0.221925            0.254011           0.278075  \n",
       "37           0.300802            0.313280           0.325980  \n",
       "38           0.300802            0.313280           0.325980  \n",
       "39           0.266043            0.298574           0.308155  \n",
       "40           0.203209            0.253119           0.296346  \n",
       "41           0.180481            0.237968           0.296346  \n",
       "42           0.264706            0.283868           0.340909  \n",
       "43           0.264037            0.313280           0.325980  \n",
       "44           0.280303            0.313280           0.330882  \n",
       "45           0.288770            0.298574           0.315508  \n",
       "46           0.300802            0.313280           0.325980  \n",
       "47           0.294118            0.343137           0.363191  \n",
       "\n",
       "[48 rows x 45 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_results.to_excel(\"output.xlsx\", sheet_name='Sheet_name_1')  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "536656feb4e5b8426698a6ed7c3ac3e4cf6780e288841fc3960502355480ee44"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
